import requests

# -------------------------------
# ğŸ§  æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆå¤‡ç”¨æ¥å£ï¼‰
# -------------------------------
def local_model(prompt, model_name="deepseek-r1", temperature=0.7, ollama_url="http://localhost:11434"):
    """
    å‚æ•°ï¼š
    - prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
    - model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ deepseek-r1, llama3 ç­‰ï¼‰
    - temperature: åˆ›æ„å¼ºåº¦ï¼ˆ0.0 ~ 1.0ï¼‰
    - ollama_url: Ollama æœåŠ¡åœ°å€

    è¿”å›ï¼š
    - æ¨¡å‹å›å¤æ–‡æœ¬ æˆ– é”™è¯¯ä¿¡æ¯
    """
    if not prompt:
        return "âš ï¸ æ²¡æœ‰è¾“å…¥æç¤ºè¯"

    try:
        response = requests.post(
            f"{ollama_url}/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False
            },
            timeout=30
        )
        response.raise_for_status()
        data = response.json()
        return data.get("response", "âš ï¸ æ¨¡å‹æœªè¿”å›ä»»ä½•å†…å®¹")
    except Exception as e:
        return f"âŒ æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}"
