import requests
from config import DEFAULT_OLLAMA_URL, DEFAULT_LOCAL_MODEL_NAME

# -------------------------------
# ğŸ§  æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆå¤‡ç”¨æ¥å£ï¼‰
# -------------------------------
def local_model(prompt, model_name=None, temperature=0.7, ollama_url=None):
    """
    å‚æ•°ï¼š
    - prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
    - model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ deepseek-r1, llama3 ç­‰ï¼‰
    - temperature: åˆ›æ„å¼ºåº¦ï¼ˆ0.0 ~ 1.0ï¼‰
    - ollama_url: Ollama æœåŠ¡åœ°å€ï¼ˆæœªæŒ‡å®šæ—¶ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰

    è¿”å›ï¼š
    - æ¨¡å‹å›å¤æ–‡æœ¬ æˆ– é”™è¯¯ä¿¡æ¯
    """
    if not prompt:
        return "âš ï¸ æ²¡æœ‰è¾“å…¥æç¤ºè¯"

    # ä½¿ç”¨é»˜è®¤é…ç½®é¡¹
    if ollama_url is None:
        ollama_url = DEFAULT_OLLAMA_URL
    if model_name is None:
        model_name = DEFAULT_LOCAL_MODEL_NAME

    try:
        response = requests.post(
            ollama_url,
            json={
                "model": model_name,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False
            },
            timeout=30
        )
        response.raise_for_status()
        data = response.json()
        return data.get("response", "âš ï¸ æ¨¡å‹æœªè¿”å›ä»»ä½•å†…å®¹")
    except Exception as e:
        return f"âŒ æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}"
