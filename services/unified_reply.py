import streamlit as st
from services.model_service import call_ollama, call_openai_via_relay

# -------------------------------
# ğŸ§  é€šç”¨æ¨¡å‹å›å¤æ¥å£
# è‡ªåŠ¨æ ¹æ®ä¾§è¾¹æ è®¾ç½®é€‰æ‹©æœ¬åœ°æˆ– OpenAI æ¨¡å‹
# -------------------------------
def reply(prompt: str, stability_override: float = None) -> str:
    """
    å‚æ•°ï¼š
    - prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
    - stability_override: å¯é€‰çš„æ¸©åº¦è¦†ç›–å€¼ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰

    è¿”å›ï¼š
    - æ¨¡å‹å›å¤æ–‡æœ¬
    """
    source = st.session_state.get("model_source", "local")
    stability = stability_override if stability_override is not None else st.session_state.get("stability", 0.7)

    if source == "local":
        model = st.session_state.get("local_model", "deepseek-r1")
        url = st.session_state.get("ollama_url", "http://localhost:11434/api/generate")
        return call_ollama(prompt, model, url, stability)
    else:
        model = st.session_state.get("openai_model", "gpt-3.5-turbo")
        username = st.session_state.get("user") or st.session_state.get("guest_id")
        return call_openai_via_relay(prompt, model, username, stability)
